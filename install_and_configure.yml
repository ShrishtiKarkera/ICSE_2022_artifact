---
- hosts: localhost
  gather_facts: false

  tasks:
    - name: Download apt key
      get_url:
        url: https://packages.cloud.google.com/apt/doc/apt-key.gpg
        dest: /tmp # or /etc/pki/rpm-gpg depending on the infrastructure

    - name: Add a key from a file
      ansible.builtin.apt_key:
        file: /tmp/apt-key.gpg
        state: present


    - name: Set up cuda key
      tags: experiment
      shell: |
        sudo apt-key del 7fa2af80
        sudo wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb
        sudo dpkg -i cuda-keyring_1.1-1_all.deb
        sudo apt-get update
        sudo apt-get -y install cuda-toolkit-12-4


    - name: Update the apt package index
      tags: experiment
      become: yes
      apt:
        name: "*"
        state: latest
        update_cache: yes
        force_apt_get: yes

    - name: install required modules
      tags: experiment
      become: true
      apt:
        name: ['git', 'apt-transport-https', 'ca-certificates', 'wget', 'software-properties-common', 'gnupg2', 'curl', 'make', 'gcc', 'autoconf', 'automake', 'libtool', 'g++', 'protobuf-compiler', 'python3-pip']

    - name: Install Pillow for Python3
      tags: experiment
      pip:
        name: Pillow
        executable: pip3

    - name: Install TensorFlow for Python3
      tags: experiment
      pip:
        name: TensorFlow
        executable: pip3

    - name: Install pycocotools for Python3
      tags: experiment
      pip:
        name: pycocotools
        executable: pip3

    - name: Install pandas for Python3
      tags: experiment
      pip:
        name: pandas
        executable: pip3

    - name: enable sudoless invocation for perf
      tags: experiment
      become: true
      shell: |
        sh -c echo -1 >/proc/sys/kernel/perf_event_paranoid
        sysctl -w kernel.perf_event_paranoid=-1

    - name: add apt signing key from official docker repo
      tags: experiment
      become: true
      apt_key:
        url: https://download.docker.com/linux/debian/gpg
        state: present

    - name: add docker official repository for Debian Stretch
      tags: experiment
      become: true
      apt_repository:
        repo: deb [arch=amd64] https://download.docker.com/linux/debian stretch stable
        state: present

    - name: Index new repo into the cache
      tags: experiment
      become: yes
      apt:
        name: "*"
        state: latest
        update_cache: yes
        force_apt_get: yes

    - name: install docker
      tags: experiment
      become: true
      apt:
        name: "docker-ce"
        state: latest

    - name: ensure docker deamon is running
      tags: experiment
      become: yes
      service:
        name: docker
        state: started

    - name: add user to docker group
      tags: experiment
      become: yes
      shell: |
        groupadd docker
        usermod -aG docker {{ user }}

    - name: define variable for nvidia-docker
      tags: experiment
      stat:
        path: ./nvidia-docker
      register: nvidiadocker

    - name: clone and install nvidia-docker
      tags: experiment
      when: not nvidiadocker.stat.exists
      become: true
      become_user: "{{ user }}"
      shell: |
        sudo wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb
        distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \
              && curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
              && curl -s -L https://nvidia.github.io/libnvidia-container/experimental/$distribution/libnvidia-container.list | \
                 sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
                 sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
        cd nvidia-docker
        make

    - name: clone and install nvidia-smi 470
      tags: experiment
      when: not nvidiadocker.stat.exists
      become: true
      become_user: "{{ user }}"
      shell: |
        apt-get purge nvidia-*
        apt-get update
        apt-get autoremove
        apt install libnvidia-common-470
        apt install libnvidia-gl-470
        apt install nvidia-driver-470
        cd nvidia-docker
        make

    - name: define variable for DeepLearningExamples
      tags: experiment
      stat:
        path: ./DeepLearningExamples
      register: deep_learning_examples

    - name: clone repo DeepLearningExamples
      tags: experiment
      when: not deep_learning_examples.stat.exists
      become: true
      become_user: "{{ user }}"
      shell: git clone https://github.com/NVIDIA/DeepLearningExamples.git

    # Build and configure Transformer-XL (Pytroch)
    - name: download data-set and build docker image for Pytorch's Transformer-XL
      tags: experiment
      become: true
      become_user: "{{ user }}"
      shell: |
        cp configs/pytorch_transformer_to_wt103_base.yml DeepLearningExamples/PyTorch/LanguageModeling/Transformer-XL/pytorch/wt103_base.yaml
        cd DeepLearningExamples/PyTorch/LanguageModeling/Transformer-XL
        mkdir data || echo Already exists
        cd data
        wget --continue https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip
        unzip -q wikitext-103-v1.zip
        cd wikitext-103
        mv wiki.train.tokens train.txt
        mv wiki.valid.tokens valid.txt
        mv wiki.test.tokens test.txt
        cd ../..
        rm data/wikitext-103-v1.zip
      register: transformer_pytorch_output

    - debug:
        msg: "{{ transformer_pytorch_output.stdout_lines|list }}"
      tags: experiment

    # Build, configure, and generate train data-set corpuse.json for Transformer-XL (Tensorflow)
    - name: download data-set and build docker image for TensorFlow's Transformer-XL
      tags: experiment
      become: true
      become_user: "{{ user }}"
      shell: |
        cp configs/tensorflow_transformer_to_run_wt103_base.sh DeepLearningExamples/TensorFlow/LanguageModeling/Transformer-XL/tf/run_wt103_base.sh
        cd DeepLearningExamples/TensorFlow/LanguageModeling/Transformer-XL
        mkdir data || echo Already exist
        cd data
        wget --continue https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip
        unzip -q wikitext-103-v1.zip
        cd wikitext-103
        mv wiki.train.tokens train.txt
        mv wiki.valid.tokens valid.txt
        mv wiki.test.tokens test.txt
        cd ../..
        rm data/wikitext-103-v1.zip
        bash tf/scripts/docker/build.sh
        docker run --tty --rm --gpus 1 --init --network=host  --ipc=host -v $PWD:/workspace/transformer-xl transformer-xl bash run_wt103_base.sh train_data --batch_chunk 32 --train_batch_size 16
      register: transformer_tensorflow_output

    - debug:
        msg: "{{ transformer_tensorflow_output.stdout_lines|list }}"
      tags: experiment

    # # Build and configure GNMT (Pytroch)
    # - name: download data-set and build docker image for Pytorch's GNMT
    #   tags: experiment
    #   become: true
    #   become_user: "{{ user }}"
    #   shell: |
    #     cp configs/pytorch_gnmt_to_be_renamed_as_train.py DeepLearningExamples/PyTorch/Translation/GNMT/train.py
    #     cd DeepLearningExamples/PyTorch/Translation/GNMT
    #     bash scripts/docker/build.sh
    #     docker run \
    #       --gpus 1 \
    #       --init --tty --rm \
    #       --network=host \
    #       --ipc=host \
    #       -v $PWD:/workspace/gnmt/ \
    #       gnmt bash scripts/wmt16_en_de.sh \
    #   register: gnmt_tensorflow_output

    # - debug:
    #     msg: "{{gnmt_tensorflow_output.stdout_lines|list }}"
    #   tags: experiment

    # # Build and configure NCF (Pytroch)
    # - name: download data-set and build docker image for Pytorch's NCF
    #   tags: experiment
    #   become: true
    #   become_user: "{{ user }}"
    #   shell: |
    #     cp configs/pytorch_ncf_to_be_renamed_as_ncf.py DeepLearningExamples/PyTorch/Recommendation/NCF/ncf.py 
    #     cd DeepLearningExamples/PyTorch/Recommendation/NCF
    #     rm -rf data || echo data dir not found
    #     mkdir data
    #     curl http://files.grouplens.org/datasets/movielens/ml-20m.zip --output ./data/ml-20m.zip && ./prepare_dataset.sh ml-20m data
    #   register: ncf_pytorch_output

    # - debug:
    #     msg: "{{ ncf_pytorch_output.stdout_lines|list }}"
    #   tags: experiment

    # # Build and configure NCF (TensorFlow)
    # - name: download data-set and build docker image for TensorFlow's NCF
    #   tags: experiment
    #   become: true
    #   become_user: "{{ user }}"
    #   shell: |
    #     cp configs/tensorflow_ncf_to_be_renamed_as_ncf.py DeepLearningExamples/TensorFlow/Recommendation/NCF/ncf.py 
    #     cd DeepLearningExamples/TensorFlow/Recommendation/NCF
    #     rm -rf data || echo data dir not ounf
    #     mkdir data
    #     curl http://files.grouplens.org/datasets/movielens/ml-20m.zip --output ./data/ml-20m.zip && ./prepare_dataset.sh ml-20m data
    #   register: ncf_tensorflow_output

    # - debug:
    #     msg: "{{ ncf_tensorflow_output.stdout_lines|list }}"
    #   tags: experiment
   
    # # Build and configure SSD (PyTorch) 
    # - name: download dataset and build docker image for PyTorch's SSD
    #   tags: experiment
    #   become: true
    #   become_user: "{{ user }}"
    #   shell: |
    #     cp configs/pytorch_ssd_SSD300_FP16_1GPU.sh DeepLearningExamples/PyTorch/Detection/SSD/examples/SSD300_FP16_1GPU.sh
    #     cd DeepLearningExamples/PyTorch/Detection/SSD
    #     mkdir coco2017 || echo Already exists
    #     bash download_dataset.sh ./coco2017
    #   register: ssd_pytorch_output

    # - debug:
    #     msg: "{{ ssd_pytorch_output.stdout_lines|list }}"
    #   tags: experiment

    # # Build and configure SSD (TensorFlow) 
    # - name: download dataset and build docker image for TensorFlows's SSD
    #   tags: experiment
    #   become: true
    #   become_user: "{{ user }}"
    #   shell: |
    #     cp configs/tensorflow_ssd_ssd320_full_1gpus.config DeepLearningExamples/TensorFlow/Detection/SSD/configs/ssd320_full_1gpus.config
    #     cp configs/tensorflow_ssd_SSD320_FP16_1GPU.sh DeepLearningExamples/TensorFlow/Detection/SSD/examples/SSD320_FP16_1GPU.sh
    #     cd DeepLearningExamples/TensorFlow/Detection/SSD
    #     rm -rf coco2017 checkpoints || echo dirs not found
    #     mkdir coco2017 checkpoints
    #     sed -i 's/COPY\ qa\/\ qa\///g' Dockerfile
    #     docker build . -t nvidia_ssd
    #     bash download_all.sh nvidia_ssd $PWD/coco2017 $PWD/checkpoints    
    #   register: ssd_tensorflow_output

    # - debug:
    #     msg: "{{ ssd_tensorflow_output.stdout_lines|list }}"
    #   tags: experiment
    
    # # Build and configure MaskRCNN (PyTorch) 
    # - name: download dataset and build docker image for PyTorch's MaskRCNN
    #   tags: experiment
    #   become: true
    #   become_user: "{{ user }}"
    #   shell: |
    #     cp configs/pytorch_mask_r_cnn_e2e_mask_rcnn_R_50_FPN_1x_1GPU.yaml DeepLearningExamples/PyTorch/Segmentation/MaskRCNN/pytorch/configs
    #     cp configs/pytorch_mask_r_cnn_paths_catalog.py DeepLearningExamples/PyTorch/Segmentation/MaskRCNN/pytorch/maskrcnn_benchmark/config/paths_catalog.py
    #     cd DeepLearningExamples/PyTorch/Segmentation/MaskRCNN
    #     cp -r ../../Detection/SSD/coco2017 ./data
    #   register: maskrcnn_pytorch_output

    # - debug:
    #     msg: "{{ maskrcnn_pytorch_output.stdout_lines|list }}"
    #   tags: experiment
        
    # # Build and configure MaskRCNN (TensorFlow) 
    # - name: download dataset and build docker image for TensorFlows's MaskRCNN
    #   tags: experiment
    #   become: true
    #   become_user: "{{ user }}"
    #   shell: |
    #     cd DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN
    #     cd dataset
    #     sed -i 's/python\ \$SCRIPT_DIR/python3\ \$SCRIPT_DIR/g' download_and_preprocess_coco.sh
    #     bash download_and_preprocess_coco.sh ./data
    #     cd ..
    #     python scripts/download_weights.py --save_dir=./weights
    #   register: maskrcnn_tensorflow_output

    # - debug:
    #     msg: "{{ maskrcnn_tensorflow_output.stdout_lines|list }}"
    #   tags: experiment

    # # Build and configure ResNet50 (PyTorch) 
    # - name: download dataset and build docker image for PyTorch's ResNet50
    #   tags: experiment
    #   become: true
    #   become_user: "{{ user }}"
    #   shell: |
    #     cp configs/pytorch_rn50_DGX1V_resnet50_AMP_90E.sh DeepLearningExamples/PyTorch/Classification/ConvNets/resnet50v1.5/training/AMP/DGX1V_resnet50_AMP_90E.sh
    #     cd DeepLearningExamples/PyTorch/Classification/ConvNets
    #     cp -r ../../Segmentation/MaskRCNN/download_dataset.sh ./
    #     bash download_dataset.sh coco2014
    #   register: rn_pytorch_output

    # - debug:
    #     msg: "{{ rn_pytorch_output.stdout_lines|list }}"
    #   tags: experiment

    # # Build and configure ResNet50 (TensorFlow) 
    # - name: download dataset and build docker image for TensorFlow's ResNet50
    #   tags: experiment
    #   become: true
    #   become_user: "{{ user }}"
    #   shell: |
    #     cp configs/tensorflow_rn50_DGX1_RN50_AMP_90E.sh DeepLearningExamples/TensorFlow/Classification/ConvNets/resnet50v1.5/training/DGX1_RN50_AMP_90E.sh 
    #     cd DeepLearningExamples/TensorFlow/Classification/ConvNets
    #     cp -r ../../../TensorFlow2/Segmentation/MaskRCNN/dataset ./
    #     cd dataset
    #     rm -rf data tf-models
    #     sed 's/2017/2014/g' download_and_preprocess_coco.sh 
    #     bash download_and_preprocess_coco.sh ./data
    #   register: rn_tensorflow_output

    # - debug:
    #     msg: "{{ rn_tensorflow_output.stdout_lines|list }}"
    #   tags: experiment
